<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Retrieval-Augmented Generation opens new attack surfaces â€” here's how to lock them down.">
  <title>Securing RAG Pipelines â€“ Complete Guide 2026 â€¢ Secure by DeZign</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">
  <link rel="stylesheet" href="../css/article.css">
  <link rel="icon" type="image/svg+xml" href="../favicon.svg">
  <link rel="canonical" href="https://www.securebydezign.com/articles/rag-security.html">
</head>
<body class="bg-zinc-950 text-zinc-200">
  <!-- Skip navigation -->
  <a href="#main-content" class="skip-link" data-i18n="skip_to_main">Skip to main content</a>
  <nav role="navigation" aria-label="Article navigation" data-i18n-aria-label="a11y_article_nav"
       class="border-b border-zinc-800 bg-zinc-950">
    <div class="max-w-4xl mx-auto px-6 py-5 flex items-center justify-between">
      <a href="../index.html" class="flex items-center gap-2 hover:text-emerald-400">
        <i class="fas fa-arrow-left" aria-hidden="true"></i>
        <span data-i18n="nav_back_home">Back to Home</span>
      </a>
      <select id="lang-switcher" class="lang-switcher"
              aria-label="Select language" data-i18n-aria-label="language_switcher">
        <option value="en">ðŸ‡ºðŸ‡¸ EN</option>
        <option value="es">ðŸ‡ªðŸ‡¸ ES</option>
      </select>
    </div>
  </nav>

  <article id="main-content" class="article-body max-w-4xl mx-auto px-6 py-16">
    <div class="article-meta text-emerald-400 text-sm mb-8">23 Feb 2026 â€¢ 14 min read</div>

<h1>Securing RAG Pipelines â€“ The Complete Defense Guide 2026</h1>

<p class="lead">Retrieval-Augmented Generation has become the backbone of enterprise AI deployments, but every component in the pipelineâ€”from document ingestion to vector retrieval to prompt constructionâ€”introduces attack surfaces that traditional security models never anticipated. This guide provides a systematic approach to hardening RAG architectures against data poisoning, prompt injection through retrieved context, embedding manipulation, and unauthorized knowledge extraction.</p>

<figure class="article-fig">
  <div class="diagram-wrap">
    <svg viewBox="0 0 800 420" xmlns="http://www.w3.org/2000/svg" aria-labelledby="rag-attack-surface-title" role="img">
      <title id="rag-attack-surface-title">RAG Pipeline Attack Surface Diagram</title>
      
      <!-- Background -->
      <rect width="800" height="420" fill="#27272a"/>
      
      <!-- Pipeline stages -->
      <!-- Document Ingestion -->
      <rect x="30" y="160" width="120" height="80" rx="8" fill="#27272a" stroke="#3f3f46" stroke-width="2"/>
      <text x="90" y="195" fill="#e4e4e7" font-family="system-ui" font-size="12" text-anchor="middle" font-weight="600">Document</text>
      <text x="90" y="212" fill="#e4e4e7" font-family="system-ui" font-size="12" text-anchor="middle" font-weight="600">Ingestion</text>
      
      <!-- Attack vector 1 -->
      <rect x="40" y="70" width="100" height="50" rx="6" fill="#dc262620" stroke="#dc2626" stroke-width="1.5" stroke-dasharray="4,2"/>
      <text x="90" y="92" fill="#fca5a5" font-family="system-ui" font-size="10" text-anchor="middle">Data Poisoning</text>
      <text x="90" y="106" fill="#fca5a5" font-family="system-ui" font-size="10" text-anchor="middle">Malicious Docs</text>
      <line x1="90" y1="120" x2="90" y2="160" stroke="#dc2626" stroke-width="1.5" marker-end="url(#arrowRed)"/>
      
      <!-- Chunking & Embedding -->
      <rect x="190" y="160" width="120" height="80" rx="8" fill="#27272a" stroke="#3f3f46" stroke-width="2"/>
      <text x="250" y="195" fill="#e4e4e7" font-family="system-ui" font-size="12" text-anchor="middle" font-weight="600">Chunking &</text>
      <text x="250" y="212" fill="#e4e4e7" font-family="system-ui" font-size="12" text-anchor="middle" font-weight="600">Embedding</text>
      
      <!-- Attack vector 2 -->
      <rect x="200" y="280" width="100" height="50" rx="6" fill="#dc262620" stroke="#dc2626" stroke-width="1.5" stroke-dasharray="4,2"/>
      <text x="250" y="302" fill="#fca5a5" font-family="system-ui" font-size="10" text-anchor="middle">Embedding</text>
      <text x="250" y="316" fill="#fca5a5" font-family="system-ui" font-size="10" text-anchor="middle">Manipulation</text>
      <line x1="250" y1="240" x2="250" y2="280" stroke="#dc2626" stroke-width="1.5" marker-end="url(#arrowRed)"/>
      
      <!-- Vector Store -->
      <rect x="350" y="160" width="120" height="80" rx="8" fill="#27272a" stroke="#10b981" stroke-width="2"/>
      <text x="410" y="195" fill="#e4e4e7" font-family="system-ui" font-size="12" text-anchor="middle" font-weight="600">Vector</text>
      <text x="410" y="212" fill="#e4e4e7" font-family="system-ui" font-size="12" text-anchor="middle" font-weight="600">Database</text>
      
      <!-- Attack vector 3 -->
      <rect x="360" y="70" width="100" height="50" rx="6" fill="#dc262620" stroke="#dc2626" stroke-width="1.5" stroke-dasharray="4,2"/>
      <text x="410" y="92" fill="#fca5a5" font-family="system-ui" font-size="10" text-anchor="middle">Index Tampering</text>
      <text x="410" y="106" fill="#fca5a5" font-family="system-ui" font-size="10" text-anchor="middle">ACL Bypass</text>
      <line x1="410" y1="120" x2="410" y2="160" stroke="#dc2626" stroke-width="1.5" marker-end="url(#arrowRed)"/>
      
      <!-- Retrieval & Ranking -->
      <rect x="510" y="160" width="120" height="80" rx="8" fill="#27272a" stroke="#3f3f46" stroke-width="2"/>
      <text x="570" y="195" fill="#e4e4e7" font-family="system-ui" font-size="12" text-anchor="middle" font-weight="600">Retrieval &</text>
      <text x="570" y="212" fill="#e4e4e7" font-family="system-ui" font-size="12" text-anchor="middle" font-weight="600">Ranking</text>
      
      <!-- Attack vector 4 -->
      <rect x="520" y="280" width="100" height="50" rx="6" fill="#dc262620" stroke="#dc2626" stroke-width="1.5" stroke-dasharray="4,2"/>
      <text x="570" y="302" fill="#fca5a5" font-family="system-ui" font-size="10" text-anchor="middle">Context Injection</text>
      <text x="570" y="316" fill="#fca5a5" font-family="system-ui" font-size="10" text-anchor="middle">via Retrieved Docs</text>
      <line x1="570" y1="240" x2="570" y2="280" stroke="#dc2626" stroke-width="1.5" marker-end="url(#arrowRed)"/>
      
      <!-- LLM Generation -->
      <rect x="670" y="160" width="100" height="80" rx="8" fill="#27272a" stroke="#3f3f46" stroke-width="2"/>
      <text x="720" y="195" fill="#e4e4e7" font-family="system-ui" font-size="12" text-anchor="middle" font-weight="600">LLM</text>
      <text x="720" y="212" fill="#e4e4e7" font-family="system-ui" font-size="12" text-anchor="middle" font-weight="600">Generation</text>
      
      <!-- Attack vector 5 -->
      <rect x="665" y="70" width="110" height="50" rx="6" fill="#dc262620" stroke="#dc2626" stroke-width="1.5" stroke-dasharray="4,2"/>
      <text x="720" y="92" fill="#fca5a5" font-family="system-ui" font-size="10" text-anchor="middle">Knowledge Exfil</text>
      <text x="720" y="106" fill="#fca5a5" font-family="system-ui" font-size="10" text-anchor="middle">Output Manipulation</text>
      <line x1="720" y1="120" x2="720" y2="160" stroke="#dc2626" stroke-width="1.5" marker-end="url(#arrowRed)"/>
      
      <!-- Flow arrows -->
      <defs>
        <marker id="arrowGreen" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto" markerUnits="strokeWidth">
          <path d="M0,0 L0,6 L9,3 z" fill="#10b981"/>
        </marker>
        <marker id="arrowRed" markerWidth="8" markerHeight="8" refX="7" refY="3" orient="auto" markerUnits="strokeWidth">
          <path d="M0,0 L0,6 L7,3 z" fill="#dc2626"/>
        </marker>
      </defs>
      
      <line x1="150" y1="200" x2="190" y2="200" stroke="#10b981" stroke-width="2" marker-end="url(#arrowGreen)"/>
      <line x1="310" y1="200" x2="350" y2="200" stroke="#10b981" stroke-width="2" marker-end="url(#arrowGreen)"/>
      <line x1="470" y1="200" x2="510" y2="200" stroke="#10b981" stroke-width="2" marker-end="url(#arrowGreen)"/>
      <line x1="630" y1="200" x2="670" y2="200" stroke="#10b981" stroke-width="2" marker-end="url(#arrowGreen)"/>
      
      <!-- Legend -->
      <rect x="280" y="370" width="240" height="35" rx="6" fill="#27272a" stroke="#3f3f46" stroke-width="1"/>
      <rect x="295" y="382" width="12" height="12" rx="2" fill="#dc262620" stroke="#dc2626" stroke-width="1"/>
      <text x="315" y="392" fill="#e4e4e7" font-family="system-ui" font-size="11">Attack Surface</text>
      <line x1="400" y1="388" x2="430" y2="388" stroke="#10b981" stroke-width="2" marker-end="url(#arrowGreen)"/>
      <text x="440" y="392" fill="#e4e4e7" font-family="system-ui" font-size="11">Data Flow</text>
    </svg>
  </div>
  <figcaption>RAG pipeline architecture with attack surfaces at each stageâ€”from document ingestion through LLM generation</figcaption>
</figure>

<div class="in-this-guide teaser-only">
  <strong>In this guide:</strong>
  <ul>
    <li>Document ingestion security: preventing data poisoning and malicious payload injection</li>
    <li>Vector database hardening: embedding integrity, access controls, and index protection</li>
    <li>Retrieval-time defenses: context filtering, relevance verification, and injection detection</li>
    <li>End-to-end audit architecture: logging, anomaly detection, and incident response for RAG systems</li>
  </ul>
</div>

<h2 class="with-icon"><i class="fas fa-file-import section-icon" aria-hidden="true"></i> Document Ingestion: The First Line of Defense</h2>

<p>The ingestion pipeline is where most RAG security failures begin. When your system accepts documents from external sourcesâ€”whether uploaded by users, scraped from the web, or synchronized from enterprise repositoriesâ€”each document represents a potential attack vector. Unlike traditional web applications where input validation focuses on structured data, RAG systems must parse and process complex document formats that can harbor malicious content invisible to conventional security scanners.</p>

<p>The fundamental challenge is that RAG systems are designed to extract and preserve semantic meaning from documents. An attacker doesn't need to exploit a parser vulnerabilityâ€”they simply need to craft content that, once embedded and retrieved, will manipulate the LLM's behavior. This includes hidden prompt injections embedded in document metadata, invisible Unicode characters that alter instruction parsing, and semantically poisoned content that appears legitimate but shifts model outputs toward attacker-controlled objectives.</p>

<p>NIST SP 800-218 (Secure Software Development Framework) provides the foundation here: treat all ingested content as untrusted input requiring validation, sanitization, and provenance tracking. But RAG systems demand additional controls that the framework doesn't explicitly addressâ€”embedding-level integrity verification, semantic anomaly detection, and content lineage tracking that persists through the chunking and retrieval process.</p>

<div class="pdf-only">
  <p>Implementing robust ingestion security requires a multi-layer approach. First, establish strict document type allowlists and validate file signatures rather than relying on extensions. Parse documents in isolated sandboxes with resource limits to prevent denial-of-service through malformed files. Extract text content through security-audited parsers, and maintain separation between document metadata and content throughout processing.</p>

  <p>The critical innovation for RAG security is pre-embedding content analysis. Before generating vectors, scan extracted text for injection patterns, anomalous Unicode sequences, and semantic inconsistencies. This is where signature-based detection meets behavioral analysisâ€”you're looking for content that might be legitimate text but contains instruction-like patterns targeting LLM behavior.</p>

  <div class="code-caption">Secure Document Ingestion Pipeline with Content Validation</div>
  <pre><code>import hashlib
import magic
from typing import Optional
from dataclasses import dataclass
from enum import Enum
import re

class ThreatLevel(Enum):
    CLEAN = "clean"
    SUSPICIOUS = "suspicious"
    MALICIOUS = "malicious"

@dataclass
class IngestionResult:
    document_id: str
    content_hash: str
    threat_level: ThreatLevel
    detected_patterns: list[str]
    sanitized_content: str
    metadata: dict

class SecureRAGIngestor:
    # Patterns that suggest prompt injection attempts
    INJECTION_PATTERNS = [
        r'ignore\s+(previous|above|all)\s+instructions',
        r'you\s+are\s+now\s+[a-z]+\s+mode',
        r'system\s*:\s*',
        r'<\|im_start\|>',
        r'\[INST\]',
        r'###\s*(instruction|system)',
        r'forget\s+(everything|what)\s+',
        r'new\s+instructions?\s*:',
    ]
    
    ALLOWED_MIME_TYPES = {
        'application/pdf',
        'text/plain',
        'text/markdown',
        'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
    }
    
    def __init__(self, sandbox_executor, embedding_model):
        self.sandbox = sandbox_executor
        self.embedder = embedding_model
        self.pattern_cache = [re.compile(p, re.IGNORECASE) for p in self.INJECTION_PATTERNS]
    
    def ingest_document(
        self, 
        file_bytes: bytes, 
        source_metadata: dict,
        user_context: dict
    ) -> Optional[IngestionResult]:
        # Step 1: Validate file type by magic bytes, not extension
        mime_type = magic.from_buffer(file_bytes, mime=True)
        if mime_type not in self.ALLOWED_MIME_TYPES:
            self._log_rejection(source_metadata, f"Blocked MIME type: {mime_type}")
            return None
        
        # Step 2: Generate content hash for integrity tracking
        content_hash = hashlib.sha256(file_bytes).hexdigest()
        
        # Step 3: Extract text in sandboxed environment
        extracted = self.sandbox.extract_text(
            file_bytes,
            timeout_seconds=30,
            max_memory_mb=512
        )
        if not extracted.success:
            self._log_rejection(source_metadata, f"Extraction failed: {extracted.error}")
            return None
        
        # Step 4: Scan for injection patterns
        detected_patterns = self._scan_for_injections(extracted.text)
        threat_level = self._assess_threat_level(detected_patterns)
        
        # Step 5: Sanitize content - remove high-risk patterns while preserving semantics
        sanitized = self._sanitize_content(extracted.text, detected_patterns)
        
        # Step 6: Detect anomalous Unicode sequences
        unicode_anomalies = self._detect_unicode_attacks(sanitized)
        if unicode_anomalies:
            detected_patterns.extend(unicode_anomalies)
            threat_level = max(threat_level, ThreatLevel.SUSPICIOUS, key=lambda x: x.value)
        
        # Step 7: Log with full provenance
        self._audit_log(
            action="document_ingested",
            content_hash=content_hash,
            source=source_metadata,
            user=user_context,
            threat_assessment={
                "level": threat_level.value,
                "patterns": detected_patterns
            }
        )
        
        return IngestionResult(
            document_id=self._generate_doc_id(content_hash, source_metadata),
            content_hash=content_hash,
            threat_level=threat_level,
            detected_patterns=detected_patterns,
            sanitized_content=sanitized,
            metadata=self._build_secure_metadata(source_metadata, user_context)
        )
    
    def _scan_for_injections(self, text: str) -> list[str]:
        detected = []
        for pattern in self.pattern_cache:
            matches = pattern.findall(text)
            if matches:
                detected.append(f"injection_pattern:{pattern.pattern[:50]}")
        return detected
    
    def _detect_unicode_attacks(self, text: str) -> list[str]:
        anomalies = []
        # Check for bidirectional override characters
        bidi_chars = ['\u202A', '\u202B', '\u202C', '\u202D', '\u202E', '\u2066', '\u2067', '\u2068', '\u2069']
        for char in bidi_chars:
            if char in text:
                anomalies.append(f"bidi_override:{hex(ord(char))}")
        
        # Check for homoglyph attacks (Cyrillic/Greek lookalikes)
        homoglyph_ranges = [(0x0400, 0x04FF), (0x0370, 0x03FF)]  # Cyrillic, Greek
        latin_text = any(c.isalpha() and ord(c) < 128 for c in text)
        mixed_scripts = any(
            any(start <= ord(c) <= end for start, end in homoglyph_ranges)
            for c in text
        )
        if latin_text and mixed_scripts:
            anomalies.append("potential_homoglyph_attack")
        
        return anomalies
    
    def _sanitize_content(self, text: str, detected_patterns: list[str]) -> str:
        sanitized = text
        # Remove bidirectional overrides
        for char in ['\u202A', '\u202B', '\u202C', '\u202D', '\u202E']:
            sanitized = sanitized.replace(char, '')
        
        # Normalize whitespace that could hide injections
        sanitized = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', sanitized)
        
        return sanitized</code></pre>

  <p>The code above demonstrates several critical security patterns. The MIME type validation uses magic bytes rather than file extensions, preventing trivial bypass attempts. The injection pattern scanner uses compiled regular expressions for efficiency while catching common prompt injection signatures. The Unicode attack detection identifies bidirectional override characters that could make malicious instructions invisible in document previews.</p>

  <div class="code-caption">Content Provenance Tracking for Audit Compliance</div>
  <pre><code>from datetime import datetime, timezone
import json

class ProvenanceTracker:
    """
    Maintains cryptographic chain of custody for all ingested content.
    Enables post-incident forensics and compliance reporting.
    """
    
    def __init__(self, storage_backend, signing_key):
        self.storage = storage_backend
        self.signer = signing_key
    
    def record_ingestion(
        self,
        document_id: str,
        content_hash: str,
        source_chain: list[dict],
        processing_steps: list[dict]
    ) -> str:
        provenance_record = {
            "document_id": document_id,
            "content_hash": content_hash,
            "ingestion_timestamp": datetime.now(timezone.utc).isoformat(),
            "source_chain": source_chain,  # Full path: original source -> transformations
            "processing_steps": processing_steps,
            "schema_version": "1.0"
        }
        
        # Sign the record for tamper detection
        record_bytes = json.dumps(provenance_record, sort_keys=True).encode()
        signature = self.signer.sign(record_bytes)
        
        provenance_record["signature"] = signature.hex()
        
        # Store with content-addressable key
        provenance_id = self.storage.store(
            key=f"provenance:{document_id}",
            value=provenance_record,
            ttl_days=2555  # 7-year retention for compliance
        )
        
        return provenance_id
    
    def verify_chain(self, document_id: str) -> dict:
        """Verify integrity of entire document processing chain."""
        record = self.storage.get(f"provenance:{document_id}")
        if not record:
            return {"valid": False, "error": "No provenance record found"}
        
        # Verify signature
        signature = bytes.fromhex(record.pop("signature"))
        record_bytes = json.dumps(record, sort_keys=True).encode()
        
        if not self.signer.verify(record_bytes, signature):
            return {"valid": False, "error": "Signature verification failed"}
        
        return {"valid": True, "record": record}</code></pre>
</div>

<div class="article-cta teaser-only text-center my-10">
  <a href="https://buy.stripe.com/aFadR8gDw2jm4UM6atb7y00" class="inline-flex items-center gap-3 bg-emerald-600 hover:bg-emerald-500 text-white px-8 py-4 rounded-2xl font-semibold transition">
    <i class="fas fa-credit-card"></i> Buy Guide for $27
  </a>
</div>

<h2 class="with-icon"><i class="fas fa-database section-icon" aria-hidden="true"></i> Vector Database Security: Protecting the Knowledge Core</h2>

<p>The vector database sits at the heart of every RAG system, storing the embedded representations that determine what information gets retrieved. A compromised vector store doesn't just leak dataâ€”it enables attackers to manipulate retrieval results, inject malicious context into every query, and potentially poison the model's outputs across your entire user base. Yet most organizations apply the same security posture to their vector databases as they would to a simple cache, missing critical attack vectors unique to embedding-based systems.</p>

<p>Traditional database security focuses on access control, encryption at rest, and query authorization. Vector databases require all of these plus additional controls for embedding integrity verification, similarity search manipulation prevention, and namespace isolation that respects document-level access policies. When a user queries the RAG system, the retrieval process must enforce the same access controls as if they were directly accessing the source documentsâ€”but vector similarity search doesn't naturally respect document permissions.</p>

<p class="teaser-note teaser-only"><em>The full guide includes detailed vector database hardening configurations, embedding integrity verification implementations, and access control patterns that maintain security without destroying retrieval performance.</em></p>

<div class="pdf-only">
  <p>The embedding integrity problem deserves special attention. Unlike traditional databases where you can verify data integrity through checksums, embeddings are high-dimensional floating-point vectors where small perturbations might be indistinguishable from legitimate variations introduced by model updates. An attacker who gains write access to the vector store could subtly shift embeddings to make certain documents more or less likely to be retrieved for specific queriesâ€”a form of availability attack that's nearly impossible to detect through standard monitoring.</p>

  <p>Implementing proper vector database security requires thinking in terms of four layers: transport security, access control, embedding integrity, and retrieval authorization. Let's examine each with practical implementation patterns.</p>

  <div class="code-caption">Vector Database Access Control with Document-Level Authorization</div>
  <pre><code>from typing import Optional
from dataclasses import dataclass
import numpy as np

@dataclass
class RetrievalContext:
    user_id: str
    roles: set[str]
    department: str
    clearance_level: int
    session_id: str

@dataclass 
class SecureDocument:
    doc_id: str
    embedding: np.ndarray
    content_preview: str
    access_policy: dict
    classification_level: int

class SecureVectorStore:
    """
    Vector store wrapper that enforces document-level access control
    during similarity search operations.
    """
    
    def __init__(self, vector_db_client, policy_engine, audit_logger):
        self.db = vector_db_client
        self.policy = policy_engine
        self.audit = audit_logger
    
    def secure_search(
        self,
        query_embedding: np.ndarray,
        context: RetrievalContext,
        top_k: int = 10,
        similarity_threshold: float = 0.7
    ) -> list[SecureDocument]:
        """
        Perform similarity search with post-retrieval access filtering.
        
        Note: We retrieve more candidates than requested, then filter.
        This prevents information leakage about document existence.
        """
        # Retrieve expanded candidate set (3x requested to account for filtering)
        candidates = self.db.similarity_search(
            embedding=query_embedding,
            limit=top_k * 3,
            include_metadata=True
        )
        
        authorized_results = []
        denied_count = 0
        
        for candidate in candidates:
            # Check access policy for each document
            access_decision = self.policy.evaluate(
                subject=context,
                resource=candidate.metadata.get("access_policy", {}),
                action="read"
            )
            
            if access_decision.allowed:
                if candidate.score >= similarity_threshold:
                    authorized_results.append(SecureDocument(
                        doc_id=candidate.id,
                        embedding=candidate.embedding,
                        content_preview=candidate.metadata.get("preview", ""),
                        access_policy=candidate.metadata.get("access_policy"),
                        classification_level=candidate.metadata.get("classification", 0)
                    ))
            else:
                denied_count += 1
                # Log denial without revealing document content
                self.audit.log_access_denied(
                    user_id=context.user_id,
                    session_id=context.session_id,
                    doc_id=candidate.id,
                    reason=access_decision.reason
                )
            
            if len(authorized_results) >= top_k:
                break
        
        # Security metric: high denial rate might indicate policy misconfiguration
        # or attempted unauthorized access
        if denied_count > top_k * 2:
            self.audit.log_anomaly(
                event_type="high_retrieval_denial_rate",
                user_id=context.user_id,
                denied_count=denied_count,
                severity="medium"
            )
        
        return authorized_results
    
    def store_with_policy(
        self,
        doc_id: str,
        embedding: np.ndarray,
        content: str,
        access_policy: dict,
        source_metadata: dict
    ) -> bool:
        """Store embedding with associated access policy and integrity metadata."""
        
        # Generate embedding integrity hash
        embedding_hash = self._hash_embedding(embedding)
        
        metadata = {
            "access_policy": access_policy,
            "preview": content[:200] if len(content) > 200 else content,
            "source": source_metadata,
            "embedding_hash": embedding_hash,
            "stored_at": datetime.now(timezone.utc).isoformat(),
            "classification": access_policy.get("classification_level", 0)
        }
        
        self.db.upsert(
            id=doc_id,
            embedding=embedding.tolist(),
            metadata=metadata
        )
        
        self.audit.log_storage(
            doc_id=doc_id,
            embedding_hash=embedding_hash,
            policy=access_policy
        )
        
        return True
    
    def _hash_embedding(self, embedding: np.ndarray) -> str:
        """
        Create integrity hash of embedding vector.
        Uses quantized representation to be robust to floating-point variations.
        """
        # Quantize to 16-bit integers for stable hashing
        quantized = (embedding * 32767).astype(np.int16)
        return hashlib.sha256(quantized.tobytes()).hexdigest()[:16]
    
    def verify_embedding_integrity(self, doc_id: str) -> dict:
        """Verify stored embedding hasn't been tampered with."""
        record = self.db.get(doc_id)
        if not record:
            return {"valid": False, "error": "Document not found"}
        
        current_hash = self._hash_embedding(np.array(record.embedding))
        stored_hash = record.metadata.get("embedding_hash")
        
        if current_hash != stored_hash:
            self.audit.log_anomaly(
                event_type="embedding_integrity_failure",
                doc_id=doc_id,
                expected_hash=stored_hash,
                actual_hash=current_hash,
                severity="critical"
            )
            return {"valid": False, "error": "Embedding integrity check failed"}
        
        return {"valid": True, "hash": current_hash}</code></pre>

  <p>The secure search implementation above demonstrates a critical pattern: post-retrieval filtering with expanded candidate retrieval. This approach prevents information leakageâ€”if we simply returned "access denied" for unauthorized documents, an attacker could infer which documents exist for specific queries. By retrieving more candidates than needed and filtering silently, we maintain consistent response sizes regardless of authorization outcomes.</p>

  <p>The embedding integrity verification uses quantized hashing to handle floating-point representation variations while still detecting malicious modifications. This is essential for detecting tampering that might occur through compromised backup restoration, insider threats with database access, or sophisticated attacks that modify embeddings to influence retrieval patterns.</p>
</div>

<h2 class="with-icon"><i class="fas fa-shield-halved section-icon" aria-hidden="true"></i> Retrieval-Time Defenses: Stopping Injection at the Gate</h2>

<p>Even with hardened ingestion and secure vector storage, the retrieval phase presents unique attack opportunities. When retrieved documents are assembled into the LLM's context window, any prompt injection payloads embedded in those documents activate. This is the indirect prompt injection attack vector that has compromised production RAG systems across industriesâ€”an attacker doesn't need to directly interact with your AI; they just need to poison a document that gets retrieved.</p>

<p>The defense requires multiple overlapping controls: context filtering that strips suspicious patterns before LLM submission, semantic verification that validates retrieved content actually relates to the user's query, output monitoring that detects when the LLM's response diverges from the expected pattern.</p>
  </article>
  <script src="rag-security-stripe.js"></script>

  <script src="../js/i18n.js" defer></script>
</body>
</html>